import requests
from bs4 import BeautifulSoup
import csv
import time
import random
import logging

# Logging Configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# List of proxy to use
proxies_list = [
    "http://your_proxy_address_1",
    "http://your_proxy_address_2",
    "http://your_proxy_address_3"
]

def google_search(query, num_results=100):
    search_url = "https://www.google.com/search"
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36"
    ]
    urls = []
    
    for start in range(0, num_results, 10):
        params = {"q": query, "start": start}
        
        user_agent = random.choice(user_agents)
        headers = {"User-Agent": user_agent}
        
        retry_attempts = 0
        success = False

        while not success and retry_attempts < 5:
            proxy = {"http": random.choice(proxies_list), "https": random.choice(proxies_list)}
            
            try:
                response = requests.get(search_url, params=params, headers=headers, proxies=proxy, timeout=10)
                response.raise_for_status()
                success = True
                
            except requests.exceptions.HTTPError as http_err:
                logging.error(f"HTTP error occurred: {http_err}")
                if response.status_code == 429:
                    wait_time = (2 ** retry_attempts) + random.uniform(0, 1)
                    logging.info(f"Rate limit exceeded. Waiting for {wait_time} seconds before retrying.")
                    time.sleep(wait_time)
                retry_attempts += 1
                
            except requests.exceptions.RequestException as err:
                logging.error(f"Error occurred: {err}")
                wait_time = (2 ** retry_attempts) + random.uniform(0, 1)
                logging.info(f"Retrying in {wait_time} seconds.")
                time.sleep(wait_time)
                retry_attempts += 1
        
        if not success:
            logging.error(f"Failed to retrieve results after {retry_attempts} attempts.")
            continue
        
        soup = BeautifulSoup(response.text, 'html.parser')
        search_results = soup.find_all('div', class_='yuRUbf')
        
        for result in search_results:
            link = result.find('a')['href']
            urls.append(link)
        
        logging.info(f"Retrieved {len(urls)} URLs so far.")
        
        time.sleep(random.randint(10, 20))
    
    return urls

def save_to_csv(urls, filename):
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["URLs"])
        for url in urls:
            writer.writerow([url])
    logging.info(f"Saved {len(urls)} URLs to {filename}")

if __name__ == "__main__":
    query = input("Enter the search query: ")
    num_results = 100
    filename = "google_search_results.csv"
    
    urls = google_search(query, num_results)
    save_to_csv(urls, filename)
    logging.info(f"Total {len(urls)} URLs found and saved.")
